def encode(df_categorical,to_drop=to_drop):
    """
    converts categorical features into numerical
    - to_drop - a list of columns you want to drop
    """
    enc = OneHotEncoder(handle_unknown='error',drop='if_binary')
    enc.fit(df_categorical)
    encoded_arr = enc.transform(df_categorical).toarray()
    encoded_cols = enc.get_feature_names_out()
    training_df = pd.DataFrame(encoded_arr,columns=encoded_cols)
    training_df.drop(columns=to_drop,inplace=True,errors='ignore')
    return training_df

def train_test(df,continuous_vars,train_size=0.67):
    """
    # creates a train test split with 67% training data, 33% test data
    - continuous_vars - concatenate these variables onto your categorical table
    """
    XY=encode(df)
    y=XY.iloc[:,-1]
    X=XY.iloc[:,:-1]
    print('X Shape: {0}'.format(X.shape))
    print('y Shape: {0}'.format(y.shape))
    X = pd.concat((X.reset_index(drop=True),continuous_vars.reset_index(drop=True)),axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-train_size),random_state=101)
    X_train=np.array(X_train)
    X_test=np.array(X_test)
    y_train=np.array(y_train)
    y_test=np.array(y_test)
    return X_train, X_test, y_train, y_test

def oversample(X_train, y_train,continuous_vars,cat_cols=False):
    """Oversamples the underrepresented population"""
    if cat_cols==False:
        cat_feat_cols = [x for x in range(X_train.shape[1]-(len(continuous_vars)))]
    else:
        cat_feat_cols=cat_cols
    # Oversample the training data
    sm = SMOTENC(random_state=42, categorical_features=cat_feat_cols)
    X_train, y_train = sm.fit_resample(X_train, y_train)
    return X_train, y_train

def model_fit(X_train,y_train):
    # Neural network model
    mlpc=MLPClassifier()
    #mlpc=LogisticRegression()
    # fit data to it
    mlpc.fit(X_train,y_train)
    return mlpc

def model_scoring(mlpc,X_test,y_test,visualise=True):
    # create a prediction
    y_predict = mlpc.predict(X_test)
    if visualise==True:
        #create the confusion matrix
        conf_mtrx = confusion_matrix(y_test,y_predict)
        # display our confusion matrix
        cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mtrx, display_labels = [False, True])
        cm_display.plot()
        plt.show()
    # How often the model is correct
    Accuracy = metrics.accuracy_score(y_test, y_predict)
    # The percentage of true positives
    Precision = metrics.precision_score(y_test, y_predict)
    # How good is model at predicting positives
    Sensitivity_recall = metrics.recall_score(y_test, y_predict)
    # How good is model at predicting negatives
    Specificity = metrics.recall_score(y_test, y_predict, pos_label=0)
    # F1 score - "harmonic mean" of precision and sensitivity
    F1_score = metrics.f1_score(y_test, y_predict)
    #print('Accuracy: {0}\nPrecision: {1}\nSensitivity: {2}\nSpecificity: {3}\nF1 Score: {4}'.format(Accuracy,Precision,Sensitivity_recall,Specificity,F1_score))
    return [Accuracy,Precision,Sensitivity_recall,Specificity,F1_score]
