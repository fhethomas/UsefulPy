# Beautiful soup, requests and pandas (pandas used here for csv wrangling)
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re # needed for regular expression to strip out grammar


def extract_hrefs(url,class_target="courseAToZ__list",print_outputs=False):
    """
    This function is intended to return a list of all hrefs from a particular webpage within a list
    Inputs:
    url - The target url, str
    class_target - the class string which contains the list, str
    Output:
    output_list - a list [course_name, course_url]
    """
    response = requests.get(url)
    html_content = response.text
    # Parse the HTML with Beautiful Soup
    soup = BeautifulSoup(html_content, "html.parser")
    # Locate the <ul> element with class "courseAToZ__list"
    ul_element = soup.find("ul", {"class": class_target})
    # create output_list
    output_list = []
    # Ensure the <ul> exists
    if ul_element:
        # Find all <li> elements within the <ul>
        list_items = ul_element.find_all("li")
        # Extract and print the text content of each <li>
        for li in list_items:
            temp_list = []
            # get text from list
            course_name = li.get_text(strip=True)
            temp_list.append(course_name)
            if print_outputs:
                print(course_name)
            a_tag = li.find("a")  # Find the <a> tag inside the <li>
            if a_tag:
                href = a_tag.get("href")  # Get the href attribute
                temp_list.append(href)
                if print_outputs:
                    print("Link:", href)
            output_list.append(temp_list)
    else:
        print("No <ul> with class 'courseAToZ__list' found.")
    return output_list

root_url = "https://www.ntu.ac.uk/study-and-courses/undergraduate/course-a-z?result_1908621_result_page="
alphabet = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')

# Create list of each course that we're going to save
final_list = []
for a in alphabet:
    # create our url from our root_url
    url = root_url+a
    print(url)
    # Call extract_hrefs on our url and add it to our final_list
    final_list += extract_hrefs(url,class_target="courseAToZ__list")

df = pd.DataFrame(final_list,columns=['Course_Title','Course_Href'])
df.drop_duplicates(inplace=True)
output_file_name = 'Nott Trent Course List.csv'
df.to_csv('C:\\My Computer\\'+output_file_name)

def get_href_text(url,class_header_target = 'course-modules__intro',class_div_target = 'course-modules__tab-content'):
    """
    This function get the header elements and module elements of the course
    url - The target url, str
    class_header_target - the class string which contains the header, str
    class_div_target - the class string which contains the module data, str
    Output:
    output_list - a list [header information, module information]
    """
    response = requests.get(url)
    html_content = response.text
    # Parse the HTML with Beautiful Soup
    soup = BeautifulSoup(html_content, "html.parser")
    header_element = soup.find("header", {"class": class_header_target})
    div_element = soup.find("div", {"class": class_div_target})
    #header_text
    if header_element!=None:
        header_text= header_element.get_text(strip=True)
    else:
        header_text='No Value'
    #div_text - contains module data
    if div_element!=None:
        div_text= div_element.get_text(strip=True)
    else:
        div_text='No Value'
    output_list = [header_text,div_text]
    return output_list

# Iterate over dataframe and find all module headers and module info
# new outputlist we're creating
final_list = []
# loop through dataframe
m,n = df.shape # get the shape of the dataframe
for row in range(m):
    temp_list = list(df.iloc[row,:])
    url = df.iloc[row,1]
    temp_list +=get_href_text(url)
    final_list.append(temp_list)
print('Output created')

df = pd.DataFrame(final_list,columns=['Course_Title','Course_Href','Course Header','Module Info'])
df.drop_duplicates(inplace=True)
output_file_name = 'Nott Trent Course List.csv'
df.to_csv('N:\\SRDStaff\\GPS\\SPI\\Insight\\Personal Workspace\\Frank\\'+output_file_name)

# Output to CSV
df = pd.DataFrame(final_list,columns=['Course_Title','Course_Href','Course Header','Module Info'])
df.drop_duplicates(inplace=True)
output_file_name = 'Nott Trent Course List.csv'
df.to_csv('C:\\My Computer\\'+output_file_name)

# Find only AI related courses
# new outputlist we're creating
final_list = []
# loop through dataframe
m,n = df.shape # get the shape of the dataframe
for row in range(m):
    header_text = df.iloc[row,2].upper()
    header_text=re.sub(r"[^\w\s]", " ", header_text) # strip out grammar
    module_text = df.iloc[row,3].upper()
    module_text=re.sub(r"[^\w\s]", " ", module_text)
    # test for AI or artificial intelligence
    if ('AI ' in header_text) or ('AI ' in module_text) or ('ARTIFICIAL INT' in header_text) or ('ARTIFICIAL INT' in module_text):
        final_list.append(list(df.iloc[row,:]))
ai_df = pd.DataFrame(final_list,columns=['Course_Title','Course_Href','Course Header','Module Info'])
output_file_name = 'Nott Trent AI Course List.csv'
ai_df.to_csv('C:\\My Computer\\'+output_file_name)
